---
title: "final-evaluation-happy"
output: html_document
date: "2024-03-08"
---

# Assignment Info

- **Student name:** Yan Wang
- **Project title:** Exploring the Role of Deviance on Self-Concept Clarity Across the Lifespan
- **Submission date:** Mar 5 at 10:59pm

## Report plan summary

Graph 1: 
3 panels of correlational analysis: SCC to SWB across all three age groups

Graph 2:
Histogram of age distributions

Table 1:
Report the regression analysis

Distribution: 700-900 introduction, 200-300 method, 200-300 result, 250-500 discussion

Elements of code: 1. correlational coefficient and p-value; 2. descriptive data of the age distribution and age groups

Goals: Able to conduct data analysis using R. Able to visualize data in APA style correctly using R. Analysis for H1 and H2, both correlational analyses if I can use my data. If I don't have my collected data, I will just be practicing using R since the analysis content does not really matter.

# Assessment

The lists of expectations in each category are to help you understand the kinds of things I'm looking for as I grade. These **do not** map onto point values. A checked box indicates the expectation was met or exceeded. (This also helps me spot patterns across students to identify ways I can improve the syllabus and lectures for future students.)

Ratings indicate overall success for each category and loosely map onto points. Starting from 20 points, no points are lost for the highest rating, -3 for the lowest, and somewhere in between for the middle ratings. Points lost in that range are dependent on how many issues there are, how much they matter, whether it's technicality vs problem of consequence, whether it's the same core issue that lost you points in a different section, whether exceeding expectations elsewhere compensate for a small issue, whether it's something we talked about in your report plan meeting, etc. 

Additional comments are included below the ratings as needed. Final project and class grades are included at the end of the document.

**WARNING: Grades for this assignment are going to be lower than you expect!** Don't freak out. I use "specs-based grading", which works very differently from the letter-grade-to-percentage conversion you are probably used to. It's more like Harry Potter OWLs -- 19/20 is outstanding, 16-18 exceeds expectations (not a C!), 13-15 meets expectations (not an F!), etc. You're not down to T(roll) until you're in the negatives. Remember this is only 20% of your final grade!

## .Rmd/PDF match

**Expectations:**

- [x] PDF submitted to Canvas is identical to the PDF created when I knit your .Rmd
- [x] Knitting does not produce any errors or problematic warnings

**Rating:**

- [x] Perfect match
- [ ] At least one minor problem
- [ ] At least one major problem and/or multiple minor problems
- [ ] Multiple major problems
- [ ] Prohibitive error(s), missing PDF, or significant problems

*No additional comments*

## Data read-in, prep, wrangling

**Expectations:**

- [ ] Data preparation and wrangling code present (may separate files)
- [x] Intermediate datasets (if used) are functional and read in successfully
- [x] Raw or intermediate data read into .Rmd code chunk
- [ ] All code in .R files and .Rmd code chunks is sufficiently commented and comprehensible
- [x] Additional expectations set in the report plan (if any) are met

**Rating:**

- [ ] No noteworthy issues 
- [ ] At least one minor problem
- [ ] At least one major problem and/or multiple minor problems
- [x] Multiple major problems
- [ ] Prohibitive errors or significant problems

SUPER IMPORTANT! Don't use install.packages() directly. Use Require() or write a similar function to first check to see whether a package is already installed (if so, load it; if not, install and load it). You don't want to overwrite an existing installation (imagine for example that the user needs an older version of the lme4 package to run a particular depricated function, if you install lme4 it will replace it with the newest version and break that user's own code)

Your .csv of prepared data reads in successfully, but I can't find your data prep as either a separate file or in .Rmd code chunks. Looking through your commits it seems like you've maybe done prep in a few different places and then deleted those scripts from your repo, but it's hard for me to piece together. When you clean up your repo, you should be consolidating and reorganizing disparate scripts, not deleting them.

## Figures, tables, analyses

### Figures

**Expectations:**

- [x] At least 2 plots are produced in .Rmd chunks (not imported images)
- [x] Figures are different kinds of plots 
- [x] Figures follow report plan (may have inconsequential deviations)
- [ ] Figures are publication-quality
- [x] Figures are sensibly captioned with `fig.cap` chunk option
- [ ] Figures are informative and interpretable
- [ ] All code in .R files and .Rmd code chunks is sufficiently commented and comprehensible
- [x] Additional expectations set in the report plan (if any) are met

**Rating:**

- [ ] No noteworthy issues 
- [ ] At least one minor problem
- [x] At least one major problem and/or multiple minor problems
- [ ] Multiple major problems
- [ ] Prohibitive errors or significant problems

Two comments on Figure 2, one minor and one major. 

Minor: to be "publication-quality", remove the redundant legend on the side of the figure. Using redundant color is helpful, the adding a legend makes your reader assume that there's an additional piece of the plot that needs to be interpreted. 

Major: Something has gone very wrong mapping data to this plot. This figure shows a *perfect* positive correlation for all three groups, with every single point for all groups on that perfect correlation line. You mention Figure 2 in your text, but you don't interpret it at all. Imagine -- how would you interpret this figure for your reader? What aspects of the plot would you want to call attention to? This is a good example of why it's really important to never just "drop" a figure. If you force yourself to give your reader a clear interpretation of the plot you will also clarify for yourself where problems arise.

### Table

**Expectations:** 

If you included more than the one required table, only one needs to meet expectations.

- [x] At least 1 table is produced in .Rmd chunk
- [ ] Table follows report plan (may have inconsequential deviations)
- [ ] Table is publication-quality
- [x] Table is sensibly captioned within the function (e.g., the kable caption argument)
- [ ] Table is informative and interpretable
- [ ] All code in .R files and .Rmd code chunks is sufficiently commented and comprehensible
- [x] Additional expectations set in the report plan (if any) are met

**Rating:**

- [ ] No noteworthy issues 
- [x] At least one minor problem
- [ ] At least one major problem and/or multiple minor problems
- [ ] Multiple major problems
- [ ] Prohibitive errors or significant problems

Plotting a corr matrix instead of regression output. That's a valid choice for a table to include (though not what we talked about) but if you go this route you need the table to be much easier to read. Things that will help are widening the whole table, adding significance stars or bolding/coloring significant correlations, and making the abbreviation key more prominent and organized.

### Analysis

**Expectations:**

If you included more than the one required analysis (or other requirements agreed on in the report plan), only one needs to meet expectations.

- [x] At least 1 statistical analysis is executed in .Rmd chunk (or sourced code if appropriate)
- [x] Analysis follows report plan (may have inconsequential deviations)
- [ ] Analysis is executed correctly
- [x] Analysis is appropriate (not necessarily ideal) for data type(s) and research questions
- [ ] Analysis is referenced and interpreted in narrative text
- [x] All code in .R files and .Rmd code chunks is sufficiently commented and comprehensible
- [x] Additional expectations set in the report plan (if any) are met

**Rating:**

- [ ] No noteworthy issues 
- [ ] At least one minor problem
- [x] At least one major problem and/or multiple minor problems
- [ ] Multiple major problems
- [ ] Prohibitive errors or significant problems

Since you ended up keeping your analyses simple (which is totally fine!), it's all the more important to be sure to use "sanity checks" (visualizing the correlations should have put up a red flag!) and to include comprehensive textual interpretation.

## Manuscript

### Narrative text

**Expectations:**

- [x] Narrative text includes 1500+ words distributed across 4 sections
- [x] Sufficient literature review (per report plan)
- [x] Sufficient methods section (per report plan)
- [ ] Sufficient results section (per report plan)
- [x] Sufficient discussion (per report plan)
- [ ] Narrative provides sufficient context and interpretation for all figures, tables, and analyses
- [x] Additional expectations set in the report plan (if any) are met

**Rating:**

- [ ] No noteworthy issues 
- [ ] At least one minor problem
- [x] At least one major problem and/or multiple minor problems
- [ ] Multiple major problems
- [ ] Prohibitive errors or significant problems

Same issues as above, relevant here too (but don't worry I'm not taking off points for the same things twice!)

### Markdown

**Expectations:**

- [x] Figures and tables are successfully referenced with markdown syntax
- [x] At least 2 in-text code references
- [x] At least 1 in-text code reference includes in-text function
- [x] R Markdown is used effectively to follow APA format (e.g., headings, emphasized text)
- [x] YAML header is functional and complete
- [x] Additional expectations set in the report plan (if any) are met

**Rating:**

- [x] No noteworthy issues 
- [ ] At least one minor problem
- [ ] At least one major problem and/or multiple minor problems
- [ ] Multiple major problems
- [ ] Prohibitive errors or significant problems

You can use in-text r code even more! You're only using it in your methods, but it's most useful in results.

### BibTeX

**Expectations:**

- [x] At least 10 in-text citations using BibTeX reference syntax
- [x] In-text citations are rendered without errors
- [x] In-text citations have no obvious problems (e.g., incorrect special characters)
- [x] R and R packages are cited in-text using `cite_r()`
- [x] References page renders without errors
- [x] References page is APA formatted and free of obvious problems
- [x] Additional expectations set in the report plan (if any) are met

**Rating:**

- [x] No noteworthy issues 
- [ ] At least one minor problem
- [ ] At least one major problem and/or multiple minor problems
- [ ] Multiple major problems
- [ ] Prohibitive errors or significant problems

Tiny issue in line 90; showing up as "-Light (2024)" when you want just the year

## GitHub & best practices

These expectations will not have an impact on your grade unless there are egregious issues, but I include them to give you feedback on what you can do if you use this workflow going forward.

**Expectations:**

- [ ] GitHub repo and contained files are sensibly and informatively named
- [ ] GitHub repo is well-structured and tidy
- [ ] GitHub README.md file is informative
- [ ] Commit messages are informative
- [x] Style is (relatively) consistent
- [ ] Comments are frequent and informative
- [x] Code chunks have informative names, follow 1-chunk-1-thing rule, and are distributed throughout the text

*No additional comments*

# Grade

- **Final report grade:** 12
- **Class participation grade:** 18
- **Weekly assignments:** 60
- **TOTAL Final course grade:** 90 (B+)
